import numpy as np
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler

#1. data  #.현재 /하단
path = './_data/ddarung/'  #data의 위치 표시 (train) , path라는 이름으로 subnission, test, train 공통점 만들어 귀찮은 일 안 만듦
train_csv = pd.read_csv(path+'train.csv', index_col=0)  # train_csv이라는 변수
# train_csv = pd.read_csv('./_data/ddarung/train.csv', index_col=0) # index_col=0 처리 안 하면 id 열로 인식함
test_csv = pd.read_csv(path+'test.csv', index_col=0)
# print(train_csv)
# print(train_csv.shape) # (1459,10) 실질적 input_dim=9 count = y
submission = pd.read_csv(path + 'submission.csv', index_col=0)


train_csv = train_csv.dropna()

x = train_csv.drop(['count'], axis = 1)
# print(x) #(1459rows,9columns)
y = train_csv['count']
# print(y)
# print(y.shape)
print(np.unique(y, return_counts=True))


x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.7)
print(x_test.shape)  #(,) (399, 9)
print(x_train.shape)  #(,) (929, 9)
print(y_train.shape)  #(,)
print(y_test.shape)  #(,) (

scaler = MinMaxScaler()
scaler.fit(x_train )
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)
test_csv = scaler.transform(test_csv)


x_test = x_test.reshape(399, 9,1)
x_train = x_train.reshape(929, 9,1)

from tensorflow.keras.layers import LSTM

model = Sequential()
model.add(LSTM(64, input_shape=(9,1)))
model.add(Dense(64, activation='relu'))
model.add(Dense(1))

#3.compile, fit
model.compile(loss='mse',optimizer='adam')
model.fit(x_train,y_train,epochs=2,batch_size=1,validation_split=0.1)

#4. evaluate
loss = model.evaluate(x_test,y_test)
y_predict = model.predict(x_test)

print(y_predict)
def RMSE(y_test, y_predict):
    return np.sqrt(mean_squared_error(y_test, y_predict))  
print("RMSE : ", RMSE(y_test,y_predict))
  
r2= r2_score(y_test,y_predict)
print("R2 : ", r2)



"""
RMSE :  62.32417573516535
R2 :  0.4154324689959229
RMSE :  62.32417573516535
"""
