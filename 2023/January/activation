activation

종류 : linear, sigmoid, relu 

1. linear - y=wx+b 

2. sigmoid - x의 값에 따라 0~1의 값을 출력하는 S자형 함수

3. relu - 입력값이 음수이면 0으로 출력한다. 입력값이 양수이면 그대로 출력한다.(w>0)

* activation 기능을 통해 오차를 조절해 함수를 예측하는데 도움을 준다.
* default 값은 linear임


다중분류 최종 output layer에 무조건 softmax를 사용한다.
*softmax의 원리는 모든 확률의 합은 무조건 1(100%)가 된다.



