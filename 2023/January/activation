activation

종류 : linear, sigmoid, relu 

1. linear - y=wx+b 

2. sigmoid - x의 값에 따라 0~1의 값을 출력하는 S자형 함수

3. relu - 입력값이 음수이면 0으로 출력한다. 입력값이 양수이면 그대로 출력한다.(w>0)

* activation 기능을 통해 오차를 조절해 함수를 예측하는데 도움을 준다.
* default 값은 linear임


다중분류 최종 output layer에 무조건 softmax를 사용한다.
*softmax의 원리는 모든 확률의 합은 무조건 1(100%)가 된다.

ex)
output이 3인  model의 경우   o    o    o     (0,     1,     2) 

첫 input부터 수 많은 layer를 거치고 0 또는 1 또는 2 위치로 들어가게 된다. 
전체 input으로 들어온 수로 각 각 0, 1, 2의 들어온 확률을 구하고 다 더하게 되면 100%가 된다.
또한, 실제로 만약 2에 들어간 수가 많다면 input x가 2일 확률이 가장 높다.

